{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6c89994",
   "metadata": {},
   "source": [
    "### 03 Bag of Words - BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4e88643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words 모델은 문서가 가지는 모든 단어(Words)를 문맥이나 순서를 무시하고 \n",
    "# 일괄적으로 단어에 대해 빈도 값을 부여해 피처 값을 추출하는 모델이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a61022c",
   "metadata": {},
   "source": [
    "### BOW 피처 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7de2335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 머신러닝 알고리즘은 일반적으로 숫자형 피처를 데이터로 입력받아 동작하기 떄문에 텍스트와 같은 데이터는 머신러닝 알고리즘에 바로 입력할 수 없다.\n",
    "# 따라서 텍스트는 특정 의미를 가지는 숫자형 값인 벡터 값으로 변환해야 하는데, 이러한 변환을 피처 벡터화라고 한다.\n",
    "# 피처 벡터화는 각 문서의 텍스트를 단어로 추출해 피처로 할당하고, 각 단어의 발생 빈도와 같은 값을 이 피처에 값으로 부여해\n",
    "# 각 문서를 이 단어 피처의 발생 빈도 값으로 구성된 벡터로 만드는 기법이다.\n",
    "# 피처 벡터화는 기존 테스트 데이터를 또 다른 형태의 피처 조합으로 변경하기 떄문에 넓은 범위의 피처 추출에 포함된다.\n",
    "\n",
    "# BOW 모델에서 피처 벡터화를 수행한다는 것은 모든 문서에서 모든 단어를 컬럼 형태로 나열하고 각 문서에서 해당 단어의 횟수나\n",
    "# 정규화된 빈도를 값으로 부여하는 데이터 세트 모델로 변경하는 것이다.\n",
    "# 예를 들어 M개의 텍스트 문서가 있고, 이 문서에서 모든 단어를 추출해 나열했을 때 N개의 단어가 있다고 가정하면\n",
    "# 문서의 피처 벡터화를 수행하면 M개의 문서는 각각 N개의 값이 할당된 피처의 벡터 세트가 된다.\n",
    "# 결과적으로는 M X N 개의 단어 피처로 이루어진 행렬을 구성하게 된다.\n",
    "\n",
    "# BOW의 피처 벡터화는 두 가지 방식이 있다.\n",
    "# * 카운트 기반의 벡터화\n",
    "# * TF-IDF(Term Frequency - Inverse Document Frequency) 기반의 벡터화\n",
    "\n",
    "# 단어 피처에 값을 부여할 때 각 문서에서 해당 단어가 나타나는 횟수, 즉 Count를 부여하는 경우를 카운트 벡터화라고 한다.\n",
    "# 카운트 벡터화에서는 카운트 값이 높을수록 중요한 단어로 인식된다.\n",
    "# 그러나 카운트만 부여할 경우 그 문서의 특징을 나타내기보다는 언어의 특성상 문장에서 자주 사용될 수 밖에 없는 단어까지 높은 값을 부여하게 된다.\n",
    "# 이러한 문제를 보완하기 위해 TF-IDF 벡터화를 사용한다.\n",
    "\n",
    "# TF-IDF는 개별 문서에서 자주 나타내는 단어에 높은 가중치를 주되, 모든 문서에서 전반적으로 자주 나타나는 단어에 대해서는 페널티를 주는\n",
    "# 방식으로 값을 부여한다.\n",
    "\n",
    "# 만일 어떤 문서에서 특정 단어가 자주 나타나면 그 단어는 해당 문서를 특정짓는 중요 단어일 수 있다.\n",
    "# 하지만, 그 단어가 다른 문서에도 자주 나타나는 단어라면 해당 단어는 언어 특성상 범용적으로 자주 사용되는 단어일 가능성이 높다\n",
    "# 모든 문서에서 반복적으로 자주 발생하는 다넝에 대해서는 페널티를 부여하는 방식으로 단어에 대한 가중치의 균형을 맞추는 것이다.\n",
    "\n",
    "# 문서마다 텍스트가 길고 문서의 개수가 많은 경우 카운트 방식보다는 TF-IDF 방식을 사용하는 것이 더 좋은 예측 성능을 보장할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2b157d",
   "metadata": {},
   "source": [
    "### 사이킷런의 Count 및 TF-IDF 벡터화 구현: CountVectorizer, TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e69f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이킷런의 CountVectorizer 클래스를 이용해 카운트 기반의 피처 여러 개의 문서로 구성된 테스트의 피처 벡터화 방법은 다음과 같다.\n",
    "\n",
    "# 첫째, 영어의 경우 모든 문자를 소문자로 변경하는 등의 전처리 작업을 수행한다.\n",
    "# 둘째, 디폴트로 단어 기준으로 n_gram_range를 반영해 각 단어를 토큰화 한다.\n",
    "# 셋째, 텍스트 정규화를 수행한다.\n",
    "\n",
    "# Stemming과 Lemmatization 같은 어근 변환은 CountVectorizer에서 직접 지원하진 않으나\n",
    "# tokenizer 파라미터에 커스텀 어근 변환 함수를 적용하여 어근 변환을 수행할 수 있다.\n",
    "# max_df, min_df, max_features등의 파라미터를 이용해 토근화된 단어를 피처로 추출하고 단어 빈도수 벡터 값을 적용한다.\n",
    "\n",
    "# 사전 데이터 가공: 모든 문자를 소문자로 변환하는 등의 사전 작업 수행(Default로 lowercase=True임).\n",
    "# 토큰화: (Default는 단어 기준(analyzer=True)이며 n_gram_range를 반영하여 토큰화 수행.\n",
    "# 텍스트 정규화: Stop Words 필터링만 수행.\n",
    "# 피처 벡터화: max_df, min_df, max_features 등의 파라미터를 반영하여 Token된 단어들을 feature extraction 후 vectorization 적용.\n",
    "\n",
    "# TF-IDF 벡터화는 TfidfVectorizer 클래스를 이용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b30178",
   "metadata": {},
   "source": [
    "### BOW 벡터화를 위한 희소 행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf4f045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 좀 더 난이도가 있는 ML 모델을 수립하기 위해서는 이러한 희소 행렬이 어떤 형태로 돼 있는지 알아야 한다.\n",
    "# 모든 문서에 있는 단어의 중복을 제거하고 피처로 만들면 일반적으로 수만 개에서 수십만 개의 단어가 만들어진다.\n",
    "# 이러한 대규모의 행렬이 생성되더라도 레코드의 각 문서가 가지는 단어의 수는 제한적이기 때문에 이 행렬의 값은 대부분 0이 차지할 수 밖에 없다.\n",
    "# 이처럼 대규모 행렬의 대부분의 값을 0이 차지하는 행렬을 가리켜 희소 행렬이라고 한다.\n",
    "# BOW 형태를 가진 언어 모델의 피처 벡터화는 대부분 희소 행렬이다.\n",
    "\n",
    "# 이 희소 행렬은 너무 많은 불필요한 0 값이 메모리 공간에 할당되어 메모리 공간이 많이 필요하며,\n",
    "# 행렬의 크기가 커서 연산 시에도 데이터 엑세스를 위한 시간이 많이 소모된다.\n",
    "# 따라서, 이러한 희소 행렬을 물리적으로 적은 메모리 공간을 차지할 수 있도록 변환해야 하는데, 대표적인 방법으로 COO 형식과 CSR 형식이 있다.\n",
    "# 일반적으로 큰 희소 행렬을 저장하고 계산을 수행하는 능력이 CSR 형식이 더 뛰어나기 떄문에 CSR을 많이 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c739fc",
   "metadata": {},
   "source": [
    "### 희소 행렬 - COO 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef5dfa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COO(Coordinate:좌표) 형식은 0이 아닌 데이터만 별도의 데이터 배열(Array)에 저장하고, \n",
    "# 그 데이터가 가리키는 행과 열의 위치를 별도의 배열로 저장하는 방식이다.\n",
    "\n",
    "# 파이썬에서는 희소 행렬 변환을 위해서 주로 사이파이(Scipy)를 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78e5a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이파이의 sparse를 이용해 희소 행렬 변환을 COO 형식으로 수행해보자.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "dense = np.array([[3, 0, 1], [0, 2, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6f1c78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# 0이 아닌 데이터 추출\n",
    "data = np.array([3, 1, 2])\n",
    "\n",
    "# 행의 위치와 열 위치를 각각 array로 생성\n",
    "row_pos = np.array([0,0,1])\n",
    "col_pos = np.array([0,2,1])\n",
    "\n",
    "# sparse 패키지의 coo_matrix를 이용하여 coo 형식으로 희소 행렬 생성\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dca004e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_coo.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9923fef2",
   "metadata": {},
   "source": [
    "### 희소 행렬 - CSR 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba7858be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSR(Compressed Sparse Row) 형식은 COO 형식이 행과 열의 위치를 나타내기 위해서 \n",
    "# 반복적인 위치 데이터를 사용해야 하는 문제점을 해결한 방식이다.\n",
    "\n",
    "# 행 위치 배열이 0부터 순차적으로 증가하는 값으로 이뤄졌다는 특성을 고려하면 행 이치 배열의 고유한 값의 시작 위치만 \n",
    "# 표기하는 방법으로 이러한 반복을 제거할 수 있다.\n",
    "# 행 위치 배열 내에 있는 고유한 값의 시작 위치만 다시 별도의 위치 배열로 가지는 변환 방식을 의미한다.\n",
    "\n",
    "# 고유 값의 시작 위치만 알고 있으면 얼마든지 행 위치 배열을 다시 만들 수 있기에 COO 방식보다 메모리가 적게 들고 빠른 연산이 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d80f14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "CSR 변환된 데이터가 제대로 된었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# CSR 방식의 변환은 사이파이의 csr_matrix 클래스를 이용해 쉽게 할 수 있다.\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "dense2 = np.array([[0,0,1,0,0,5],\n",
    "              [1,4,0,3,2,5],\n",
    "              [0,6,0,3,0,0],\n",
    "              [2,0,0,0,0,0],\n",
    "              [0,0,0,7,0,8],\n",
    "              [1,0,0,0,0,0]])\n",
    "\n",
    "# 0 이 아닌 데이터 추출\n",
    "data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성\n",
    "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
    "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
    "\n",
    "# COO 형식으로 변환\n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos, col_pos)))\n",
    "\n",
    "# 행 위치 배열의 고유한 값드의 시작 위치 인덱스를 배열로 생성\n",
    "row_pos_ind = np.array([0, 2, 7, 9 ,10, 12, 13])\n",
    "\n",
    "# CSR 형식으로 변환\n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
    "\n",
    "print('COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_coo.toarray())\n",
    "print('\\n')\n",
    "print('CSR 변환된 데이터가 제대로 된었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2027aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense3 = np.array([[0,0,1,0,0,5],\n",
    "                   [1,4,0,3,2,5],\n",
    "                   [2,0,0,0,0,0],\n",
    "                   [0,0,0,7,0,8],\n",
    "                   [1,0,0,0,0,0]])\n",
    "\n",
    "coo = sparse.coo_matrix(dense3)\n",
    "csr = sparse.csr_matrix(dense3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90294e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이킷런의 CountVectorizer나 TfidVectorizer 클래스로 변환된 피처 벡터화 행렬은 모두 사이파이의 CSR 형태의 희소 행렬이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
